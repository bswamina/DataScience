---
title: "Practical Machine Learning - Prediction Project"
author: 'Bhaskar S'
date: '25th Jun 2016'
output: 
  html_document: 
    keep_md: yes
---
<br/>

## __Predict Manner of Exercise__
<br/>

### __Introduction__
<br/>

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

The goal of this project is to predict the manner in which they did the exercise. This is the `classe` variable in the training set. We may use any of the other variables to predict with. This report describes how we get and clean the data, build the different models, how we used cross validation to test the models, and how we made our final model choice. We will also use our prediction model to predict 20 different test cases.

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har.

<br/>
### __Getting Data__
<br/>

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

We will download the training and test __csv__ files from the above mentioned locations into the directory __~/Downloads/DATA__ and load into the data in these files as `data.frame` objects  `training.data` and `testing.data` respectively.

```{r proj_1, echo = TRUE, warning = FALSE, message = FALSE}
### Initialize the url and file variables
training.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training.csv <- "~/Downloads/DATA/pml-training.csv"
testing.csv <- "~/Downloads/DATA/pml-testing.csv"

### Check and create the directrory
if (!dir.exists('~/Downloads/DATA')) {
    dir.create('~/Downloads/DATA', recursive=TRUE)
}

### Download the csv files
if (!file.exists(training.csv)) {
    download.file(training.url, destfile=training.csv)
}
if (!file.exists(testing.csv)) {
    download.file(testing.url, destfile=testing.csv)
}

### Clean up
rm(training.url, testing.url)

### Load the csv files as data.frame objects
training.data <- read.csv(training.csv)
testing.data <- read.csv(testing.csv)

### Clean up
rm(training.csv, testing.csv)

### How many rows and columns
dim(training.data)
```

From the above, we observe that the `training.data` data set contains `r dim(training.data)[1]` rows and `r dim(training.data)[2]` columns.

<br/>
### __Cleaning Data__
<br/>

Often times a given data set will contain predictors that will take a set of unique constant values (zero variance) across the sample. Such predictors (called the near zero variance predictors) will not be useful in prediction and should be removed.

We will use the __nearZeroVar__ method from the __caret__ package to identify the near zero variance predictors from our `training.data`.

```{r proj_2, echo = TRUE, warning = FALSE, message = FALSE}
### Load the caret package
library(caret)

### Identify near zero variance predictors (if any)
zero.var <- nearZeroVar(training.data, saveMetrics=TRUE)

### List the names of the near zero variance predictors
names(training.data)[zero.var$nzv]
```

From the above, it is clear we have quite a few predictors that have near zero variance.

We will remove all the predictors that have near zero variance and save the results in a new `data.frame` called `training.0`.

```{r proj_3, echo = TRUE, warning = FALSE, message = FALSE}
### Identify columns indexes of predictors to remove
zero.var.index <- nearZeroVar(training.data, saveMetrics=FALSE)

### Remove some predictors that add no value
training.0 <- training.data[, -zero.var.index]

### Clean up
rm(zero.var, zero.var.index, training.data)
```

Next, we need to identify predictors that have missing values as they add no value in the modeling.

```{r proj_4, echo = TRUE, warning = FALSE, message = FALSE}
### Identify columns with missing values
colnames(training.0)[colSums(is.na(training.0)) > 0]
```

From the above, it is clear we have quite a few predictors that have missing values.

We will remove all the predictors that have missing values and save the results in a new `data.frame` called `training.1`.

```{r proj_5, echo = TRUE, warning = FALSE, message = FALSE}
### Identify columns indexes of predictors to remove
na.index <- c()
for (i in 1:ncol(training.0)) {
    if (sum(is.na(training.0[,i])) >= nrow(training.0)/2) {
        na.index <- c(na.index, i)
    }
}

### Remove some predictors with missing values
training.1 <- training.0[, -na.index]

### Clean up
rm(na.index, training.0)
```

Let us look at the first few rows to see if there is any other clean to be done.

```{r proj_6, echo = TRUE, warning = FALSE, message = FALSE}
### Display first 5 rows and all columns
head(training.1)
```

From the above, we observe that the first six columns are just ids and times. These columns are not going to be useful in prediction and hence we can remove them. We will save the results in a new `data.frame` called `training.2`.

```{r proj_7, echo = TRUE, warning = FALSE, message = FALSE}
### Remove the first 6 columns
training.2 <- training.1[, -c(1, 2, 3, 4, 5, 6)]

### Clean up
rm(training.1)
```

<br/>
### __Exploratory Analysis__
<br/>

Now that we have cleaned the data set, the next step is to identify if there are any correlated predictors.

```{r proj_8, echo = TRUE, warning = FALSE, message = FALSE, fig.path = './figures/', fig.width = 10, fig.height = 8}
### Load the corrplot package
library(corrplot)

### Create a correlation matrix - we are not interested in the sign
cor.matrix <- abs(cor(training.2[,-53]))

### Predictors will be strongly related to themselves - zero the diagonal
diag(cor.matrix) <- 0

### Plot the correlation
corrplot(cor.matrix)
```

From the above plot it is clear that there are some predictors that are highly correlated to some others. Predictors that are highly correlated increase the noise and reduce the accuracy of the model. Hence, it is better to remove the highly correlated predictors. We will save the results in a new `data.frame` called `training.3`.

```{r proj_9, echo = TRUE, warning = FALSE, message = FALSE}
### Identify the index of the predictors that are highly correlated
cor.index <- findCorrelation(cor.matrix, cutoff=0.8)

### Remove the predictors that are corellated
training.3 <- training.2[, -cor.index]

### Clean up
rm(cor.matrix, cor.index, training.2)

### How many rows and columns
dim(training.3)
```

At this point we have a clean data set and from the above, we observe that the `training.3` data set contains `r dim(training.3)[1]` rows and `r dim(training.3)[2]` columns.

<br/>
### __Predictive Modeling__
<br/>

Since `classe` is a categorical variable with 5 distict values - __A, B, C, D, E__, this is a classification problem vs a regression problem. For the classification modeling, we will try the __Decision Tree__, __Random Forest__, and the __Gradient Boost__ supervised machine learning algorithms.

For any good modeling approach, one needs to have a traing data set and a validation data set. We will use the data in `training.3` to create a __60/40__ split of the training set and validation set.

```{r proj_10, echo = TRUE, warning = FALSE, message = FALSE}
### Set the seed for reproducibility
set.seed(1000)

### Create a 60% training set and 40% validation set
inTrain <- createDataPartition(training.3$classe, p=0.6)[[1]]
training.set <- training.3[inTrain,]
validation.set <- training.3[-inTrain,]

### Clean up
rm(inTrain, training.3)
```

To prevent the model from overfitting (which can result in prediction errors and reduced accuracy), we use the k-folds cross validation during model building. In this case we use a 10-fold cross validation.

```{r proj_11, echo = TRUE, warning = FALSE, message = FALSE}
### 10-fold cross validation
train.control <- trainControl(method='cv', number=10)
```

<br/>
#### __Predictive Modeling - Decision Tree__
<br/>

We will start modeling with a __Decision Tree__ algorithm using the `training.set` data.

```{r proj_12, echo = TRUE, warning = FALSE, message = FALSE}
### Train the model using decision tree
model.rpart <- train(classe ~ ., method='rpart',
                     trControl=train.control, data=training.set)
```

The following plot depicts the decision tree in a pictorial view:

```{r proj_13, echo = TRUE, warning = FALSE, message = FALSE, fig.path = './figures/', fig.width = 10, fig.height = 8}
### Load the rattle package
library(rattle)

### Display the decision tree
fancyRpartPlot(model.rpart$finalModel)
```

Now, we predict the outcome using the decision tree model `model.rpart` we just created on the `validation.set` data and determine the accuracy of the prediction using the confusion matrix.

```{r proj_14, echo = TRUE, warning = FALSE, message = FALSE}
### Predict the outcome on the validation set
predict.rpart <- predict(model.rpart, validation.set)

### Determine the accuracy of prediction
confusionMatrix(predict.rpart, validation.set$classe)$overall[1]
```

<br/>
#### __Predictive Modeling - Random Forest__
<br/>

We will next train a model with the __Random Forest__ algorithm using the `training.set` data.

```{r proj_15, echo = TRUE, warning = FALSE, message = FALSE}
### Load the randomForest package
library(randomForest)

### Train the model using random forest
model.rf <- randomForest(classe ~ ., data=training.set)
```

Now, we predict the outcome using the random forest model `model.rf` we just created on the `validation.set` data and determine the accuracy of the prediction using the confusion matrix.

```{r proj_16, echo = TRUE, warning = FALSE, message = FALSE}
### Predict the outcome on the validation set
predict.rf <- predict(model.rf, validation.set)

### Determine the accuracy of prediction
confusionMatrix(predict.rf, validation.set$classe)$overall[1]
```

<br/>
#### __Predictive Modeling - Gradient Boost__
<br/>

We will finally train a model with the __Gradient Boost__ algorithm using the `training.set` data.

```{r proj_17, echo = TRUE, warning = FALSE, message = FALSE}
### Train the model using gradient boost
gbm.out <- capture.output(
    model.gbm <- train(classe ~ ., method='gbm',
                       trControl=train.control, data=training.set)
)
rm(gbm.out)
```

Now, we predict the outcome using the gradient boost model `model.gbm` we just created on the `validation.set` data and determine the accuracy of the prediction using the confusion matrix.

```{r proj_18, echo = TRUE, warning = FALSE, message = FALSE}
### Predict the outcome on the validation set
predict.gbm <- predict(model.gbm, validation.set)

### Determine the accuracy of prediction
confusionMatrix(predict.gbm, validation.set$classe)$overall[1]
```

We will now summarize the accuracy of the three models.

```{r proj_19, echo = TRUE, warning = FALSE, message = FALSE, fig.path = './figures/'}
### Plot the algorithms vs accuracy
algorithms <- c('Decision Tree', 'Random Forest', 'Gradient Boost')
accuracy <- c(confusionMatrix(predict.rpart, validation.set$classe)$overall[1],
              confusionMatrix(predict.rf, validation.set$classe)$overall[1],
              confusionMatrix(predict.gbm, validation.set$classe)$overall[1])
prediction.results <- data.frame(algorithms=algorithms, accuracy=accuracy)
plot(prediction.results, lwd=2)
```

From the above plot, we clearly see that the __Random Forest__ algorithm is the most accurate model for prediction and hence we choose that model.
<br/>

### __Final Model Testing__
<br/>

Now that we have chosen the __Random Forest__ predictive model `model.rf` due to its high accuracy, we will predict the outcomes on the `testing.data` data set that was downloaded and set aside for final testing.

```{r proj_20, echo = TRUE, warning = FALSE, message = FALSE}
### Predict the outcome on the testing set
predict.final <- predict(model.rf, testing.data)

### Display the final outcomes
predict.final
```
